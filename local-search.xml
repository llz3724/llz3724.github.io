<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CS229: 0</title>
    <link href="/2025/04/21/2025-04-21-cs229_0/"/>
    <url>/2025/04/21/2025-04-21-cs229_0/</url>
    
    <content type="html"><![CDATA[<p>机器学习的多元微积分基础</p><span id="more"></span><h1 id="数学基础">数学基础</h1><h2 id="偏导和梯度">偏导和梯度</h2><p>考虑<span class="math inline"><em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>) : ℝ<sup><em>n</em></sup> → ℝ</span>，对某一个自变量<span class="math inline"><em>x</em><sub><em>i</em></sub></span>求导，其余参数视为常数，记为</p><p><br /><span class="math display">$$\frac {\partial f(x)}{\partial x_i}$$</span><br /></p><p>而<span class="math inline"><em>f</em>(<em>x</em>)</span>的梯度，是<span class="math inline"><em>f</em></span>在<span class="math inline"><em>x</em></span>这一点的“导数”的方向，也就是在该点附近，用线性空间中的一组基来近似，沿着梯度方向走，<span class="math inline"><em>f</em>(<em>x</em>)</span>增长最快。几何意义，就是该点等高线的切线的方向。</p><p>如何理解这个东西呢？ 在一维的时候你很难联想到导数的正负和往哪边走函数会变大这回事，如果非要这样理解，高维的时候每一维的长度又会对方向起作用，这是不能合理外推的，更迷糊了。</p><p>但是微积分的核心思想还是线性化，可以这么考虑，每一维的梯度是对函数这一维的“切片”的线性近似，梯度就是在一点将函数近似为了一个高维立方体，偏导大小指示了这一维的变换速度，就相当于把函数简化到了一个高维立方体里面考虑，就自然多了。</p><p><br /><span class="math display">$$\nabla f(x)=\left[\begin{matrix}  \frac {\partial f(x)}{\partial x_1} \\  \frac {\partial f(x)}{\partial x_2} \\  ... \\\frac {\partial f(x)}{\partial x_n}\end{matrix}\right]$$</span><br /></p><p>而<span class="math inline"><em>f</em>(<em>x</em>)</span>的二阶导，类似地，是一个矩阵（考虑每个维度两两求偏导），而一个事实是</p><p><br /><span class="math display">$$\frac {\partial f(x)}{\partial x_i \partial x_j}=\frac {\partial f(x)}{\partial x_j \partial x_i}$$</span><br /></p><p>学了数学会知道如果二者都连续的话是成立的。</p><p>所以二阶导是一个实对称的矩阵，称为海森矩阵<span class="math inline">∇<sup>2</sup> = <em>H</em></span></p><p><br /><span class="math display">$$H_{i,j}=\frac {\partial f(x)}{\partial x_i \partial x_j}$$</span><br /></p><p>如果<span class="math inline"><em>H</em>(<em>x</em><sub>0</sub>)</span>是半正定的，那么<span class="math inline"><em>f</em></span>在<span class="math inline"><em>x</em><sub>0</sub></span>是凸的。</p><p>一些常见的梯度</p><p><br /><span class="math display">$$\begin{aligned}&amp;\nabla_{x} (x^TAx)=2Ax\\&amp;\nabla_{x} (v^Tx)=v\\&amp;\nabla_A|A|=|A|(A^{-1})^T\end{aligned}$$</span><br /></p><p><span class="math inline"><em>A</em></span>是一堆数字到一个数字的映射。梯度就是泛函<span class="math inline">|<em>A</em>|</span>对每个变量求导的结果。</p><p>考虑现在对<span class="math inline"><em>a</em><sub>11</sub></span>求偏导</p><p><br /><span class="math display">$$\begin{aligned}\frac{\partial{|A|}}{\partial{a_{i,j}}}&amp;=\lim_{h\rightarrow 0}\frac{\left|\begin{matrix}a_{1,1}+h &amp; a_{1,2} &amp; ... &amp; a_{1,n}\\a_{2,1} &amp; a_{2,2} &amp; ... &amp; a_{2,n}\\ &amp;  &amp;...  &amp; \\a_{n,1} &amp; a_{n,2} &amp; ... &amp; a_{n,n}\end{matrix}\right|-\left|\begin{matrix}a_{1,1} &amp; a_{1,2} &amp; ... &amp; a_{1,n}\\a_{2,1} &amp; a_{2,2} &amp; ... &amp; a_{2,n}\\ &amp;  &amp;...  &amp; \\a_{n,1} &amp; a_{n,2} &amp; ... &amp; a_{n,n}\end{matrix}\right|}{h}\\&amp;=\lim_{h\rightarrow 0}\frac{hM}{h}\\&amp;=M\\\end{aligned}$$</span><br /></p><p>所以</p><p><br /><span class="math display">∇<sub><em>A</em></sub>|<em>A</em>| = (<em>A</em><sup>*</sup>)<sup><em>T</em></sup> = |<em>A</em>|(<em>A</em><sup> − 1</sup>)<sup><em>T</em></sup></span><br /></p><h2 id="雅可比矩阵">雅可比矩阵</h2><p>考虑更广泛的函数<span class="math inline"><em>y</em> = <em>f</em>(<em>x</em>) : ℝ<sup><em>n</em></sup> → ℝ<sup><em>m</em></sup></span>，则记广义的梯度为雅可比矩阵</p><p><br /><span class="math display">$$\left[\begin{matrix}\frac{\partial{y_1}}{\partial{x_1}} &amp; \frac{\partial{y_1}}{\partial{x_2}} &amp; ... &amp; \frac{\partial{y_1}}{\partial{x_n}}\\\frac{\partial{y_2}}{\partial{x_1}} &amp; \frac{\partial{y_2}}{\partial{x_2}} &amp; ... &amp; \frac{\partial{y_2}}{\partial{x_n}}\\...\\\frac{\partial{y_m}}{\partial{x_1}} &amp; \frac{\partial{y_m}}{\partial{x_2}} &amp; ... &amp; \frac{\partial{y_m}}{\partial{x_n}}\\\end{matrix}\right]_{m\times n}$$</span><br /></p><p>注意这里<span class="math inline"><em>y</em></span>一个梯度变成行了。</p><p>然后还要定义对矩阵求导<span class="math inline"><em>y</em> = <em>f</em>(<em>A</em>) : ℝ<sup><em>n</em>, <em>n</em></sup> → ℝ</span></p><p><br /><span class="math display">$$\nabla_{A}=\left[\begin{matrix}\frac{\partial{y}}{\partial{a_{1,1}}} &amp; \frac{\partial{y}}{\partial{a_{1,2}}} &amp; ... &amp; \frac{\partial{y}}{\partial{a_{1,n}}}\\\frac{\partial{y}}{\partial{a_{2,1}}} &amp; \frac{\partial{y}}{\partial{a_{2,2}}} &amp; ... &amp; \frac{\partial{y}}{\partial{a_{2,n}}}\\...\\\frac{\partial{y}}{\partial{a_{n,1}}} &amp; \frac{\partial{y}}{\partial{a_{n,2}}} &amp; ... &amp; \frac{\partial{y}}{\partial{a_{n,n}}}\\\end{matrix}\right]$$</span><br /></p><p>拉格朗日乘子之类的东西放到后面写。</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs229</tag>
      
      <tag>ai</tag>
      
      <tag>ml</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
