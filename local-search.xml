<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>大学物理: 柯尼希定理</title>
    <link href="/2025/04/22/2025-04-22-konig/"/>
    <url>/2025/04/22/2025-04-22-konig/</url>
    
    <content type="html"><![CDATA[<h1 id="柯尼希定理">柯尼希定理</h1><p>我们知道有质点系，质点系的动量是好算的，那质点系的动能和质心有什么关系？</p><p>设<span class="math inline"><em>m</em><sub><em>i</em></sub>, <em>v</em><sub><em>i</em></sub></span>是质点系第<span class="math inline"><em>i</em></span>个对象的质量、相对于地面的速度，<span class="math inline"><em>m</em><sub><em>c</em></sub>, <em>v</em><sub><em>c</em></sub></span>是质心的质量、速度，显然有</p><p><br /><span class="math display">$$K_{total}=\sum_{i}^{}\frac{1}{2}m_iv_i^2 $$</span><br /></p><p>设<span class="math inline"><em>v</em><sub><em>i</em></sub>′ = <em>v</em><sub><em>i</em></sub> − <em>v</em><sub><em>c</em></sub></span>是第<span class="math inline"><em>i</em></span>个对象相对质心的速度，代入<span class="math inline"><em>K</em><sub><em>t</em><em>o</em><em>t</em><em>a</em><em>l</em></sub></span>的定义式得</p><p><br /><span class="math display">$$K_{total}=\sum_{i}^{}\frac{1}{2}m_i(v_i'+v_c)^2=\sum_{i}^{}\frac{1}{2} m_iv_i'^2+v_c\sum_{i}^{}m_iv_i'+\sum_{}^{}\frac{1}{2}m_iv_c^2\\=K_c+\sum_{i}^{}K'_i+v_c\sum_{i}^{}m_iv_i'$$</span><br /></p><p>而质心系中，质心速度的定义是</p><p><br /><span class="math display">$$\sum_{i}^{}m_iv_i=Mv_c\\\sum_{i}^{}m_iv_i'=0$$</span><br /></p><p>所以</p><p><br /><span class="math display">$$K_{total}=K_c+\sum_{i}^{}K'_i$$</span><br /></p><p>给了我们一种新的计算总体的动能的方法。</p><p>然而和图论的柯尼希定理并没有什么关系。</p><p>一质量为<span class="math inline"><em>m</em></span>，半径为<span class="math inline"><em>R</em></span>的细圆环在地面上无摩擦以<span class="math inline"><em>ω</em></span>的角速度滚动，求圆环动能。</p><p><span class="math inline">$K_{total}=\frac{1}{2}m(\omega R)^2+\sum_{i}^{}\frac{1}{2} m_i(\omega R)^2=2\times \frac{1}{2}m\omega^2R^2=m\omega^2R^2$</span></p><h1 id="质心运动定理">质心运动定理</h1><p>与之类似的就是大物提到的质心运动定理</p><p><br /><span class="math display">$$E_k=\frac{1}{2}J_Cw^2+\frac{1}{2}m_Cv_C^2$$</span><br /></p><p>意思是，质点系的动能是绕过质心的轴转动的动能加上质心运动的动能。</p>]]></content>
    
    
    
    <tags>
      
      <tag>physics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229: 1</title>
    <link href="/2025/04/22/2025-04-22-cs229_1/"/>
    <url>/2025/04/22/2025-04-22-cs229_1/</url>
    
    <content type="html"><![CDATA[<p>线性回归</p><span id="more"></span><h1 id="多元线性回归">多元线性回归</h1><h2 id="多元线性回归的梯度下降法">多元线性回归的梯度下降法</h2><p>规定<span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span>表示训练集的第<span class="math inline"><em>i</em></span>个输入，<span class="math inline"><em>y</em><sup>(<em>i</em>)</sup></span>表示相应的答案。</p><p>设 <br /><span class="math display">$$X_{n\times d}=\left[\begin{matrix}  x_1^T \\  x_2^T \\  ... \\  x_n^T\end{matrix}\right],y_{n\times 1}=\left[\begin{matrix}  y_1 \\y_2 \\  ... \\  y_n\end{matrix}\right]$$</span><br /></p><p>首先，要意识到，不可能完美预测，所以只能追求“误差”最小。</p><p>用<span class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>)</span>表示输入<span class="math inline"><em>x</em></span>得出的预测结果。</p><p>现在考虑最简单的预测方法， <br /><span class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sup><em>T</em></sup><em>x</em></span><br /></p><p>其中在给出样本的基础上，添加一个参数<span class="math inline"><em>x</em><sub>0</sub> = 1</span>。</p><p>而且学习是离线的，即只是在已知的训练集上学习一次，用学习出的参数<span class="math inline"><em>θ</em></span>来预测。</p><p>根据统计直觉，使用欧式空间的“度量”，也就是L2距离来定义误差，即损失函数<span class="math inline"><em>J</em>(<em>θ</em>)</span>。学习的目标就是在给出的训练集上让<span class="math inline"><em>J</em>(<em>θ</em>)</span>最小。为了后续求导方便，设置一个常数<span class="math inline">$\frac{1}{2}$</span>。也有人考虑类似方差的定义，设为<span class="math inline">$\frac{1}{2n}$</span></p><p><br /><span class="math display">$$J(\theta)=\frac{1}{2}\sum_{i=1}^{n}||y^{(i)}-h_{\theta}(x^{(i)})||^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)$$</span><br /></p><p>所谓的梯度下降法就是对<span class="math inline"><em>J</em>(<em>θ</em>)</span>每个维度求偏导，也就是梯度，再根据梯度来更新<span class="math inline"><em>θ</em></span>，也就是<span class="math inline"><em>θ</em></span>向<span class="math inline"><em>J</em>(<em>θ</em>)</span>更小的方向迈一步。</p><p>当然，这只是一个很朴素的贪心算法，所以有可能会陷入局部最优解。但是后面我们可以看到线性回归其实是凸的，对于非凸的函数会进行后面一系列的改进。</p><p>求梯度</p><p><br /><span class="math display">$$\nabla_{\theta}J(\theta)=\nabla_{\theta}\frac{1}{2}(X\theta-y)^T(X\theta-y)\\=\nabla_{\theta}\frac{1}{2}(\theta^TX^T-y^T)(X\theta-y)\\=\nabla_{\theta}\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty)\\=\nabla_{\theta}\frac{1}{2}(\theta^TX^TX\theta-2y^TX\theta+y^Ty)\\=X^T(X\theta-y)$$</span><br /></p><p>那么更新<span class="math inline"><em>θ</em></span>就是</p><p><br /><span class="math display">$$\theta:=\theta-\alpha \nabla_{\theta}J(\theta)=\theta-\alpha X^T(X\theta-y)=\theta+\alpha\sum_{i=1}^{n}x^{(i)}(y^{(i)}-\theta^Tx^{(i)})$$</span><br /></p><h2 id="多元线性回归的损失函数的合理性">多元线性回归的损失函数的合理性</h2><p>一般来说，多元线性回归的求解中，先是假定了损失函数</p><p><br /><span class="math display">$$J(\theta)=\frac{1}{2}  \sum_{i=1}^{n} ||y^{(i)}-\theta ^T x^{(i)}||^2$$</span><br /></p><p>然后求导，做梯度下降更新参数。</p><p>看起来损失函数的设置来源于统计学的直觉，但是为什么对，或者说为什么合理呢？</p><p>假设</p><p><br /><span class="math display"><em>y</em><sup>(<em>i</em>)</sup> = <em>θ</em><sup><em>T</em></sup><em>x</em><sup>(<em>i</em>)</sup> + <em>ϵ</em><sub><em>i</em></sub></span><br /></p><p>而</p><p><br /><span class="math display">$$\epsilon_i \sim \N(0,\sigma^2)$$</span><br /></p><p>其中<span class="math inline"><em>σ</em></span>是定值。</p><p>我们再假设所有<span class="math inline"><em>ϵ</em><sub><em>i</em></sub></span>都是独立同分布的，也就是说所有<span class="math inline"><em>ϵ</em><sub><em>i</em></sub></span>均相互独立但都服从该分布。</p><p>至于为什么要假设符合正态分布呢？</p><p>根据中心极限定理，规定了均值和方差后，<span class="math inline"><em>n</em></span>足够大，和正态分布的误差就能任意地小。</p><p>而找到对应的<span class="math inline"><em>θ</em></span>，从概率上来讲，就是要做极大似然估计，使得每个<span class="math inline"><em>ϵ</em><sub><em>i</em></sub></span>取到自己对应的值的时候，总的概率最大</p><p><br /><span class="math display">$$L(\theta)=\prod_{i=1}^{n}P(\epsilon_i=y^{(i)}-\theta ^T x^{(i)}|x^{(i)})$$</span><br /></p><p><br /><span class="math display">$$\ln L(\theta)=\sum_{i=1}^{n}\ln P(\epsilon_i=y^{(i)}-\theta ^T x^{(i)}|x^{(i)})$$</span><br /></p><p><br /><span class="math display">$$=\sum_{i=1}^{n}\ln \frac{1}{\sigma\sqrt{2\pi}}\exp \{-\frac{(y^{(i)}-\theta ^T x^{(i)})^2}{2\sigma^2}\}$$</span><br /></p><p><br /><span class="math display">$$=\sum_{i=1}^{n}\ln \frac{1}{\sigma\sqrt{2\pi}}-\frac{(y^{(i)}-\theta ^T x^{(i)})^2}{2\sigma^2}$$</span><br /></p><p>也就是说明了损失函数的合理性。</p><h2 id="多元线性回归的解析解">多元线性回归的解析解</h2><p>设 <br /><span class="math display">$$X_{n\times d}=\left[\begin{matrix}  x_1^T \\  x_2^T \\  ... \\  x_n^T\end{matrix}\right],y_{n\times 1}=\left[\begin{matrix}  y_1 \\y_2 \\  ... \\  y_n\end{matrix}\right]$$</span><br /> 直接从定义出发，可得</p><p><br /><span class="math display">$$\nabla_{\theta} J(\theta)=-(\sum_{i=1}^n x^{(i)}_jy_j)^T+X^TX\theta=-X^Ty+X^TX\theta$$</span><br /></p><p>假设<span class="math inline"><em>X</em><sup><em>T</em></sup><em>X</em></span>可逆，则<span class="math inline"><em>θ</em></span>的解析解是</p><p><br /><span class="math display"><em>θ</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup> − 1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span><br /></p><p>当然，也可以直接考虑</p><p><br /><span class="math display">$$\nabla_{\theta} J(\theta)=\nabla_{\theta} \frac{1}{2}(X\theta-y)^T(X\theta-y)$$</span><br /></p><p>然后求解。</p><p>但是如果<span class="math inline"><em>X</em><sup><em>T</em></sup><em>X</em></span>不可逆，就要考虑其伪逆了。</p><h2 id="随机梯度下降stochastic-gradient-descent">随机梯度下降（Stochastic Gradient Descent）</h2><p>原本的梯度下降解线性回归，可以看到每进行一次梯度下降要遍历所有的点，如果数据集足够大的话，即使并行矩阵运算，耗费的时间仍然是不可接受的，也就是说，有限的时间内，我们可能并没有完成几次完整的梯度下降。</p><p>原本的梯度下降的更新方式是</p><p><br /><span class="math display">$$\theta:=\theta+\alpha\sum_{i=1}^n (y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}$$</span><br /></p><p>其中<span class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sup><em>T</em></sup><em>x</em></span>，其中<span class="math inline"><em>x</em><sub>0</sub> = 1</span>。</p><p>一个很工程但是很不严谨的想法是，不要每次都用所有点来更新，比如每次随机选一个点来更新直到收敛</p><p><br /><span class="math display"><em>θ</em> := <em>θ</em> + <em>α</em>(<em>y</em><sup>(<em>r</em>)</sup> − <em>h</em><sub><em>θ</em></sub>(<em>x</em><sup>(<em>r</em>)</sup>))<em>x</em><sup>(<em>r</em>)</sup></span><br /></p><p>这就是SGD。</p><p>或者每次选一小批点来更新，直到收敛。这就是mini-batch SGD，比SGD更常用。</p><p>比较类似概率中的“采样”，也可以认为是对训练集的一个无偏估计。如果使用以后的课的记法，可以理解为<span class="math inline"><em>α</em></span>吸收了一个系数<span class="math inline">$\frac{1}{n}$</span>，实际上原本的损失函数就是训练集梯度的平均数，SGD就是在估计平均数。</p><p>虽然不一定能得到最优的解，但是一定可以收敛，最后一般能达到一个比较优的解。</p><p>一般来说，相较于普通GD，SGD在训练前期收敛比较快，而且函数非凸时不容易陷入局部最优。</p><p>为什么SGD可以跳出局部最优呢？</p><p>如果假设高维空间中每一维相互独立，那么每一维上出现某种“形状”的概率是一样的，如果出现严格的局部最优，也就是每一维上都是一个类似抛物线的形状，或者严格地说，该点的Haissen矩阵是半正定的，每一维上是相乘的关系，高维时概率就很小了。换句话说，高维空间梯度为0的非全局最优点几乎都是鞍部，而SGD如果陷入了鞍点，更新时用部分数据来估计该点的实际梯度，实际上是带有噪声的，也就是不是严格的0，就很容易跳出鞍点。</p><p>直观理解，普通的GD，虽然每次梯度下降的效果显著，但是耗时多，不能下降几次；虽然SGD每次只对一个点进行GD，对整体而言效果不显著，但是成本低，可以进行很多次。SGD每次在训练集中选取一个很小的集合，只要训练次数足够多，就能覆盖整个训练集。</p><p>工程上，学习率<span class="math inline"><em>α</em></span>一般不会固定，会随训练时间增加而慢慢减少，类似模拟退火。</p><h2 id="局部加权线性回归">局部加权线性回归</h2><p>如果要拟合的函数正体并没有线性的特征，比如正弦函数，看起来线性回归就失效了？</p><p>但是我们可以考虑要预测的点的附近的已知点，可以认为局部就是线性的，所有可以在局部选取几个最近的点做线性回归，但坏处是变成了在线算法。</p><p>当然可以用KNN（k-近邻算法），可以k-d tree，ball tree，或者奇奇怪怪的近似算法，直接忽略太远的点。</p><p>另外一个自然的想法是修改损失函数，使得较远的点的影响减弱。</p><p><br /><span class="math display">$$J(\theta)=\sum_{i=1}^{n}w_i(y^{(i)}-\theta ^T x^{(i)})^2,0&lt;w_i&lt;1$$</span><br /></p><p>一个非常著名的<span class="math inline"><em>w</em></span>的设置被称为高斯核函数</p><p><br /><span class="math display">$$w_i=\exp\{-\frac{||x^{(i)}-x||^2}{2\tau ^2}\}$$</span><br /></p><p>其中<span class="math inline"><em>τ</em></span>是人为设定的，决定多远的已知点能起作用。</p><h2 id="梯度下降的动量法">梯度下降的动量法</h2><p>更多优化器请参考CS230笔记。</p><h2 id="k近邻算法k-nearest-neighbor-knn">K近邻算法(K-Nearest Neighbor, KNN)</h2><p>在前面的局部线性回归提到的优化算法之一，也可以用于逻辑回归（分类），在线地输入每个点，根据最近的k个点所属的类别来确定该点的类别。</p><h3 id="k-d-tree">k-d tree</h3><h3 id="ball-tree">ball tree</h3>]]></content>
    
    
    
    <tags>
      
      <tag>cs229</tag>
      
      <tag>ai</tag>
      
      <tag>ml</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229: 1</title>
    <link href="/2025/04/22/2025-04-22-cs229_2/"/>
    <url>/2025/04/22/2025-04-22-cs229_2/</url>
    
    <content type="html"><![CDATA[<p>logistic regression</p><span id="more"></span><h1 id="逻辑回归">逻辑回归</h1><p>回归解决的是<span class="math inline"><em>y</em></span>连续时的预测问题，我们还想解决<span class="math inline"><em>y</em></span>离散时的预测问题，即分类。</p><p>考虑最简单的，分成两类，即<span class="math inline"><em>y</em> ∈ {0, 1}</span></p><p>设<span class="math inline">$h_{\theta}(x)=\frac{1}{1+e^{-\theta^T x}}$</span>，称为<span class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>o</em><em>i</em><em>d</em></span>函数。这样既保留了“线性”的部分，又得到了一个被压缩到一个概率的输出。至于为什么选这个函数是恰当的，而不是其他类似<span class="math inline">arctan </span>之类的函数之后会论证。这里<span class="math inline"><em>x</em><sub>0</sub> = 1</span>。</p><p>一个小技巧是<span class="math inline"><em>g</em>′(<em>z</em>) = <em>g</em>(<em>z</em>)(1 − <em>g</em>(<em>z</em>))</span></p><p>然后还是类似前面的，做一个极大似然估计。</p><p>现在我们已知了输入，让匹配为训练集正确结果的概率最大，也就是要求<span class="math inline"><em>θ</em></span>，满足<span class="math inline">∏<em>P</em>(<em>y</em> = <em>y</em><sup>(<em>i</em>)</sup>|<em>x</em> = <em>x</em><sup>(<em>i</em>)</sup>)</span>最大。注意，这里和前面类似，要融入已知的信息<span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span>，所以是一个条件概率。</p><p>然后还要定义</p><p><br /><span class="math display">$$P(y=y^{(i)}|x=x^{(i)})=\begin{cases}h_{\theta}(x) &amp; \text{if } y^{(i)}=1\\1-h_{\theta}(x) &amp; \text{if } y^{(i)}=0\\\end{cases}$$</span><br /></p><p>我们可以巧妙地写成</p><p><br /><span class="math display"><em>P</em>(<em>y</em> = <em>y</em><sup>(<em>i</em>)</sup>|<em>x</em> = <em>x</em><sup>(<em>i</em>)</sup>) = <em>h</em><sub><em>θ</em></sub>(<em>x</em><sup>(<em>i</em>)</sup>)<sup><em>y</em><sup>(<em>i</em>)</sup></sup>(1 − <em>h</em><sub><em>θ</em></sub>(<em>x</em><sup>(<em>i</em>)</sup>))<sup>1 − <em>y</em><sup>(<em>i</em>)</sup></sup></span><br /></p><p>为什么不写成加法之类的形式呢？其实仔细观察会发现，这是伯努利分布的特例。可以理解为，线性回归的先验分布是高斯分布，而逻辑回归的先验分布是伯努利分布。当然，在之后的GLM中，可以看到他们都是一种更广义的分布的特例。</p><p><br /><span class="math display">$$L(\theta)=\prod_{i=1}^{n}P(y^{(i)}|x^{(i)})$$</span><br /></p><p><br /><span class="math display">$$l(\theta)=\ln L(\theta)=\sum_{i=1}^{n}y^{(i)}\ln h_{\theta}(x^{(i)})+(1-y^{(i)})\ln (1-h_{\theta}(x^{(i)}))$$</span><br /></p><p>右边这个也被叫做交叉熵函数</p><p><br /><span class="math display">$$\frac{\partial{l(\theta)}}{\partial{\theta_j}}=\sum_{i=1}^{n}[y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}(-1)h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))]\frac{\partial{\theta^Tx^{(i)}}}{\partial{\theta_j}}$$</span><br /></p><p><br /><span class="math display">$$\frac{\partial{l(\theta)}}{\partial{\theta_j}}=\sum_{i=1}^{n}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_j$$</span><br /></p><p>确实也可以看到这样设置概率求导比较方便。</p><p>然后就可以直接做梯度下降了。</p><p>如果沿用之前的SGD，更新规则就是</p><p><br /><span class="math display"><em>θ</em><sub><em>j</em></sub> := <em>θ</em><sub><em>j</em></sub> + <em>α</em>(<em>h</em><sub><em>θ</em></sub>(<em>x</em><sup>(<em>i</em>)</sup>) − <em>y</em><sup>(<em>i</em>)</sup>)<em>x</em><sub><em>j</em></sub><sup>(<em>i</em>)</sup>(1 ≤ <em>j</em> ≤ <em>d</em>)</span><br /></p><p>多分类、softmax函数什么的，留到GLM再一起讲。</p><h2 id="牛顿迭代法">牛顿迭代法</h2><p>如果要求函数的零点，我们知道大名鼎鼎的牛顿迭代法，即选定一个初始值<span class="math inline"><em>θ</em><sub>0</sub></span>，然后不断更新</p><p><br /><span class="math display">$$\theta:=\theta-\frac{f(\theta)}{f'(\theta)}$$</span><br /></p><p>也就是在一点附近用切线来近似改函数，然后不断修正直到收敛。</p><p>而梯度下降也是类似的，我们是在求梯度的零点，所以可以更新</p><p><br /><span class="math display">$$\theta:=\theta-\frac{f'(\theta)}{f''(\theta)}$$</span><br /></p><p>直观理解，多元函数中导数就是梯度，而二阶导就是海森矩阵，除以二阶导就是乘上海森矩阵的逆矩阵。</p><p><br /><span class="math display"><em>θ</em> := <em>θ</em> − <em>H</em><sup> − 1</sup>∇<sub><em>θ</em></sub><em>l</em>(<em>θ</em>)</span><br /></p><h2 id="感知机算法">感知机算法</h2><p>如果不考虑<span class="math inline"><em>s</em><em>i</em><em>g</em><em>m</em><em>o</em><em>i</em><em>d</em></span>，单纯</p><p><br /><span class="math display">$$g(z)=\begin{cases}1 &amp; \text{if } z\geq 0\\0 &amp; \text{if } z&lt;0\\\end{cases},h_{\theta}(x)=g(\theta^Tx)$$</span><br /></p><p>但是调用一样的<span class="math inline"><em>θ</em></span>更新方法，就得到了古老的感知机算法。</p><p>实际上和后面的svm相似，都是试图用一个超平面来分隔点。</p><p>但是区别在于，<span class="math inline"><em>s</em><em>v</em><em>m</em></span>可以魔改后可以分类出错误的点，但是一定要最大化最小的geometric margin，但是感知机算法是一定要最小化分类错误的点的个数，就可能会导致过拟合。</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs229</tag>
      
      <tag>ai</tag>
      
      <tag>ml</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS229: 0</title>
    <link href="/2025/04/21/2025-04-21-cs229_0/"/>
    <url>/2025/04/21/2025-04-21-cs229_0/</url>
    
    <content type="html"><![CDATA[<p>机器学习的多元微积分基础</p><span id="more"></span><h1 id="数学基础">数学基础</h1><h2 id="偏导和梯度">偏导和梯度</h2><p>考虑<span class="math inline"><em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>) : ℝ<sup><em>n</em></sup> → ℝ</span>，对某一个自变量<span class="math inline"><em>x</em><sub><em>i</em></sub></span>求导，其余参数视为常数，记为</p><p><br /><span class="math display">$$\frac {\partial f(x)}{\partial x_i}$$</span><br /></p><p>而<span class="math inline"><em>f</em>(<em>x</em>)</span>的梯度就是每一维偏导构成的向量，表示是<span class="math inline"><em>f</em></span>在<span class="math inline"><em>x</em></span>这一点的“导数”的方向的推广，也就是在该点附近，用线性空间中的一组基来近似，沿着梯度方向走，<span class="math inline"><em>f</em>(<em>x</em>)</span>增长最快。几何意义，就是该点等高线的切线的方向。</p><p>如何理解这个东西呢？</p><p>在一维的时候你很难联想到导数的正负和往哪边走函数会变大这回事，如果非要这样理解，高维的时候每一维的长度又会对方向起作用，这是不能合理外推的，更迷糊了。</p><p>但是微积分的核心思想还是线性化，可以这么考虑，每一维的梯度是对函数这一维的“切片”的线性近似，梯度就是在一点将函数近似为了一个高维立方体，偏导大小指示了这一维的变换速度，就相当于把函数简化到了一个高维立方体里面考虑，就自然多了。</p><p>可以用nabla算子<span class="math inline">∇</span>来表示梯度。</p><p><br /><span class="math display">$$\nabla f(x)=\left[\begin{matrix}  \frac {\partial f(x)}{\partial x_1} \\  \frac {\partial f(x)}{\partial x_2} \\  ... \\\frac {\partial f(x)}{\partial x_n}\end{matrix}\right]$$</span><br /></p><p>更广义的nabla算子定义是<span class="math inline">$\nabla=(\frac{\partial{}}{\partial{x_1}},\frac{\partial{}}{\partial{x_2}},...\frac{\partial{}}{\partial{x_n}})$</span>，可以用来表示旋度散度。</p><p>像叉积一样，都是非常自由的记号。</p><h2 id="散度divergence">散度(divergence)</h2><p>表示向量场的发散强度。</p><p><br /><span class="math display">$$div \mathbf F=\nabla\cdot \mathbf F=\frac{\partial{F_x}}{\partial{x}}+\frac{\partial{F_y}}{\partial{y}}+\frac{\partial{F_z}}{\partial{z}}$$</span><br /></p><h2 id="旋度curl">旋度(curl)</h2><p>表示向量场对微元的旋转强度。</p><p><br /><span class="math display">$$curl \mathbf v=(\frac{\partial{v_z}}{\partial{y}}-\frac{\partial{v_y}}{\partial{z}})\mathbf i+(\frac{\partial{v_x}}{\partial{z}}-\frac{\partial{v_z}}{\partial{x}})\mathbf j+(\frac{\partial{v_y}}{\partial{x}}-\frac{\partial{v_x}}{\partial{y}})\mathbf k$$</span><br /></p><p><br /><span class="math display">$$\nabla\times F=\left|\begin{matrix}\mathbf i &amp; \mathbf j &amp; \mathbf k \\\frac{\partial{}}{\partial{x}} &amp; \frac{\partial{}}{\partial{y}} &amp; \frac{\partial{}}{\partial{z}} \\F_x &amp; F_y &amp; F_z\end{matrix}\right|$$</span><br /></p><p>有一个性质，梯无旋，旋无散。</p><h2 id="二阶导与海森矩阵">二阶导与海森矩阵</h2><p>而<span class="math inline"><em>f</em>(<em>x</em>)</span>的二阶导的推广到<span class="math inline"><em>x</em></span>是向量的情况，类似地，是一个矩阵（考虑每个维度两两求偏导），而一个事实是</p><p><br /><span class="math display">$$\frac {\partial f(x)}{\partial x_i \partial x_j}=\frac {\partial f(x)}{\partial x_j \partial x_i}$$</span><br /></p><p>学了数学会知道如果二者都连续的话是成立的，机器学习里面一般不会有太特殊的函数。</p><p>所以二阶导是一个实对称的矩阵，称为海森矩阵<span class="math inline">∇<sup>2</sup> = <em>H</em></span></p><p><br /><span class="math display">$$H_{i,j}=\frac {\partial f(x)}{\partial x_i \partial x_j}$$</span><br /></p><p>如果<span class="math inline"><em>H</em>(<em>x</em><sub>0</sub>)</span>是半正定的，那么<span class="math inline"><em>f</em></span>在<span class="math inline"><em>x</em><sub>0</sub></span>是凸的。</p><p>一些常见的梯度</p><p><br /><span class="math display">$$\begin{aligned}&amp;\nabla_{x} (x^TAx)=2Ax\\&amp;\nabla_{x} (v^Tx)=v\\&amp;\nabla_A|A|=|A|(A^{-1})^T\end{aligned}$$</span><br /></p><p><span class="math inline"><em>A</em></span>是一堆数字到一个数字的映射。梯度就是泛函<span class="math inline">|<em>A</em>|</span>对每个变量求导的结果。</p><p>考虑现在对<span class="math inline"><em>a</em><sub>11</sub></span>求偏导</p><p><br /><span class="math display">$$\begin{aligned}\frac{\partial{|A|}}{\partial{a_{i,j}}}&amp;=\lim_{h\rightarrow 0}\frac{\left|\begin{matrix}a_{1,1}+h &amp; a_{1,2} &amp; ... &amp; a_{1,n}\\a_{2,1} &amp; a_{2,2} &amp; ... &amp; a_{2,n}\\ &amp;  &amp;...  &amp; \\a_{n,1} &amp; a_{n,2} &amp; ... &amp; a_{n,n}\end{matrix}\right|-\left|\begin{matrix}a_{1,1} &amp; a_{1,2} &amp; ... &amp; a_{1,n}\\a_{2,1} &amp; a_{2,2} &amp; ... &amp; a_{2,n}\\ &amp;  &amp;...  &amp; \\a_{n,1} &amp; a_{n,2} &amp; ... &amp; a_{n,n}\end{matrix}\right|}{h}\\&amp;=\lim_{h\rightarrow 0}\frac{hM}{h}\\&amp;=M\\\end{aligned}$$</span><br /></p><p>所以</p><p><br /><span class="math display">∇<sub><em>A</em></sub>|<em>A</em>| = (<em>A</em><sup>*</sup>)<sup><em>T</em></sup> = |<em>A</em>|(<em>A</em><sup> − 1</sup>)<sup><em>T</em></sup></span><br /></p><h2 id="雅可比矩阵">雅可比矩阵</h2><p>考虑更广泛的函数<span class="math inline"><em>y</em> = <em>f</em>(<em>x</em>) : ℝ<sup><em>n</em></sup> → ℝ<sup><em>m</em></sup></span>，则记广义的梯度为雅可比矩阵</p><p><br /><span class="math display">$$\left[\begin{matrix}\frac{\partial{y_1}}{\partial{x_1}} &amp; \frac{\partial{y_1}}{\partial{x_2}} &amp; ... &amp; \frac{\partial{y_1}}{\partial{x_n}}\\\frac{\partial{y_2}}{\partial{x_1}} &amp; \frac{\partial{y_2}}{\partial{x_2}} &amp; ... &amp; \frac{\partial{y_2}}{\partial{x_n}}\\...\\\frac{\partial{y_m}}{\partial{x_1}} &amp; \frac{\partial{y_m}}{\partial{x_2}} &amp; ... &amp; \frac{\partial{y_m}}{\partial{x_n}}\\\end{matrix}\right]_{m\times n}$$</span><br /></p><p>注意这里<span class="math inline"><em>y</em></span>一个梯度变成行了。</p><p>然后还要定义对矩阵求导<span class="math inline"><em>y</em> = <em>f</em>(<em>A</em>) : ℝ<sup><em>n</em>, <em>n</em></sup> → ℝ</span></p><p><br /><span class="math display">$$\nabla_{A}=\left[\begin{matrix}\frac{\partial{y}}{\partial{a_{1,1}}} &amp; \frac{\partial{y}}{\partial{a_{1,2}}} &amp; ... &amp; \frac{\partial{y}}{\partial{a_{1,n}}}\\\frac{\partial{y}}{\partial{a_{2,1}}} &amp; \frac{\partial{y}}{\partial{a_{2,2}}} &amp; ... &amp; \frac{\partial{y}}{\partial{a_{2,n}}}\\...\\\frac{\partial{y}}{\partial{a_{n,1}}} &amp; \frac{\partial{y}}{\partial{a_{n,2}}} &amp; ... &amp; \frac{\partial{y}}{\partial{a_{n,n}}}\\\end{matrix}\right]$$</span><br /></p><p>拉格朗日乘子之类的东西放到后面写。</p>]]></content>
    
    
    
    <tags>
      
      <tag>cs229</tag>
      
      <tag>ai</tag>
      
      <tag>ml</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
